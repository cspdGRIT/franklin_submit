{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission by Priyadarshan for Franklin Templeton Assignment, Copy Rights-2024\n",
    "# Loan Data Classification with Multiple Algorithms\n",
    "\n",
    "The code is designed to analyze a dataset related to loans (real estate financing) and predict whether a borrower will default on their loan. It uses various machine learning models to make these predictions based on historical data.\n",
    "\n",
    "The notebook demonstrates a complete machine learning workflow using loan data. We will:\n",
    "- Load and preprocess the data\n",
    "- Train several machine learning models\n",
    "- Evaluate model performance using key metrics\n",
    "- Use SHAP (SHapley Additive exPlanations) to explain the model predictions\n",
    "\n",
    "- Data Preparation: Start with thorough data cleaning and preprocessing to ensure quality input data.\n",
    "- Feature Engineering: Create meaningful features that can help improve model predictions.\n",
    "- Model Training and Evaluation: Use a variety of models to see which performs best on validation data.\n",
    "- Hyperparameter Tuning and Cross-Validation: Optimize models through tuning while validating their performance across different subsets of data.\n",
    "- Final Model Selection: Choose the best-performing model based on comprehensive evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "We will start by importing the necessary Python libraries, including:\n",
    "- `pandas` and `numpy` for data manipulation.\n",
    "- `scikit-learn` for model training and evaluation.\n",
    "- `shap` for model explainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     RandomForestClassifier,\n\u001b[1;32m      6\u001b[0m     GradientBoostingClassifier,\n\u001b[1;32m      7\u001b[0m     AdaBoostClassifier,\n\u001b[1;32m      8\u001b[0m     BaggingClassifier\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier  # Requires xgboost package\n",
    "from lightgbm import LGBMClassifier  # Requires lightgbm package\n",
    "from catboost import CatBoostClassifier  # Requires catboost package\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data\n",
    "\n",
    "Here, we load the loan data from two CSV files:\n",
    "- **MPLCaseStudy.csv**: Contains the loan data.\n",
    "- **mapping.csv**: Contains a mapping of column names to their descriptions.\n",
    "\n",
    "We will also rename the columns based on the descriptions provided in `mapping.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "loan_data = pd.read_csv('MPLCaseStudy.csv', low_memory=False)\n",
    "mapping = pd.read_csv('mapping.csv')\n",
    "\n",
    "# Apply column renaming from mapping.csv\n",
    "mapping_dict = dict(zip(mapping['LoanStatNew'], mapping['Description']))\n",
    "loan_data.rename(columns=mapping_dict, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "We will now preprocess the data by:\n",
    "- Converting percentage strings to floats\n",
    "- Handling date columns and creating a new feature `years_since_earliest_cr_line`.\n",
    "- Dropping irrelevant date columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Function to convert percentage strings to floats\n",
    "def convert_percent_to_float(x):\n",
    "    if isinstance(x, str) and '%' in x:\n",
    "        return float(x.strip('%')) / 100\n",
    "    return x\n",
    "\n",
    "# Convert date columns to datetime format and create new features\n",
    "date_columns = ['The month the borrower\\'s earliest reported credit line was opened',\n",
    "                'The month which the loan was funded']\n",
    "for col in date_columns:\n",
    "    loan_data[col] = pd.to_datetime(loan_data[col], errors='coerce')\n",
    "\n",
    "loan_data['years_since_earliest_cr_line'] = loan_data['The month the borrower\\'s earliest reported credit line was opened'].apply(\n",
    "    lambda x: (pd.Timestamp.now() - x).days / 365 if pd.notnull(x) else None)\n",
    "\n",
    "# Drop original date columns\n",
    "loan_data.drop(columns=date_columns, inplace=True)\n",
    "\n",
    "# Convert percentage columns to float values\n",
    "percentage_columns = [\n",
    "    'Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.',\n",
    "    'Percentage of all bankcard accounts > 75% of limit.',\n",
    "    'Balance to credit limit on all trades'\n",
    "]\n",
    "for col in percentage_columns:\n",
    "    loan_data[col] = loan_data[col].apply(convert_percent_to_float)\n",
    "\n",
    "# Handle missing 'loan_status' column\n",
    "if 'loan_status' not in loan_data.columns:\n",
    "    loan_data.rename(columns={'Current status of the loan': 'loan_status'}, inplace=True)\n",
    "\n",
    "# Identify numeric columns that need imputation and remove columns with all missing values\n",
    "numeric_columns = loan_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "loan_data_numeric = loan_data[numeric_columns].dropna(axis=1, how='all')\n",
    "\n",
    "# Impute missing values for numeric columns using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "loan_data_imputed = pd.DataFrame(imputer.fit_transform(loan_data_numeric), columns=loan_data_numeric.columns)\n",
    "loan_data.update(loan_data_imputed)\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in loan_data.select_dtypes(include=[object]):\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    loan_data[column] = label_encoders[column].fit_transform(loan_data[column].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Values and Encode Categorical Columns\n",
    "\n",
    "Next, we will:\n",
    "- Handle missing values in numeric columns by using median imputation.\n",
    "- Remove any columns that have all missing values.\n",
    "- Encode categorical columns using `LabelEncoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Handle missing 'loan_status' column\n",
    "if 'loan_status' not in loan_data.columns:\n",
    "    loan_data.rename(columns={'Current status of the loan': 'loan_status'}, inplace=True)\n",
    "\n",
    "# Identify numeric columns that need imputation\n",
    "numeric_columns = loan_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Remove columns with all missing values\n",
    "loan_data_numeric = loan_data[numeric_columns].dropna(axis=1, how='all')\n",
    "\n",
    "# Preprocessing - Handling missing values for numeric columns using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "loan_data_imputed = pd.DataFrame(imputer.fit_transform(loan_data_numeric), columns=loan_data_numeric.columns)\n",
    "\n",
    "# Reassign imputed columns back to original DataFrame\n",
    "loan_data.update(loan_data_imputed)\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for column in loan_data.select_dtypes(include=[object]):\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    loan_data[column] = label_encoders[column].fit_transform(loan_data[column].astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data for Training and Testing\n",
    "\n",
    "We will now split the data into training and testing sets with an 80/20 split, keeping the `loan_status` column as the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Split data into features and target variable, then into train and test sets\n",
    "X = loan_data.drop(columns=['loan_status'])\n",
    "y = loan_data['loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Machine Learning Models\n",
    "\n",
    "We will initialize several machine learning models to compare their performance:\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors\n",
    "- Support Vector Machine (SVM)\n",
    "- Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Initialize a list of models to compare (including new models)\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=500),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree'),  # Use BallTree for efficiency\n",
    "    'SVM': SVC(probability=True),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Bagging': BaggingClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),  # Suppress warning for XGBoost \n",
    "    'LightGBM': LGBMClassifier(),\n",
    "    'CatBoost': CatBoostClassifier(silent=True)  # Suppress output for CatBoost training \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Function to Evaluate Models\n",
    "\n",
    "This function will:\n",
    "- Train a model on the training data.\n",
    "- Predict results on the test data.\n",
    "- Print performance metrics such as accuracy, confusion matrix, classification report, and ROC AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def evaluate_model_with_timeout(model, X_train, X_test, y_train, y_test, timeout=60):\n",
    "    \"\"\"Fit the model and evaluate it using various metrics with a timeout.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if elapsed_time > timeout:\n",
    "            print(f\"Model {model.__class__.__name__} fitting exceeded timeout of {timeout} seconds.\")\n",
    "            return\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Check for probability predictions\n",
    "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            if len(np.unique(y_test)) > 2:  # Handle multi-class case\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "                print(\"ROC AUC Score (multi-class): \", roc_auc)\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "                print(\"ROC AUC Score: \", roc_auc)\n",
    "        \n",
    "        print(\"F1 Score (weighted): \", f1_score(y_test, y_pred, average='weighted'))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating model {model.__class__.__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Transformation\n",
    "\n",
    "We will use pipelines to handle both numerical and categorical features. The numeric features will be imputed using the median and scaled, while categorical features will be imputed and one-hot encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipelines for numeric and categorical data\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessor to X_train and X_test\n",
    "X_train_imputed = preprocessor.fit_transform(X_train)\n",
    "X_test_imputed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert the result back to DataFrame for interpretability (optional)\n",
    "X_train_imputed_df = pd.DataFrame(X_train_imputed.toarray() if hasattr(X_train_imputed, 'toarray') else X_train_imputed)\n",
    "X_test_imputed_df = pd.DataFrame(X_test_imputed.toarray() if hasattr(X_test_imputed, 'toarray') else X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate the models\n",
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Evaluate each model on the preprocessed data with a timeout for KNN specifically.\n",
    "for name, model in models.items():\n",
    "    evaluate_model_with_timeout(model, X_train_imputed_df, X_test_imputed_df, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: SHAP Analysis\n",
    "Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Explainable AI with SHAP for RandomForest model (or any other model of choice)\n",
    "explainer = shap.TreeExplainer(models['RandomForest'])\n",
    "shap_values = explainer.shap_values(X_test_imputed_df)\n",
    "\n",
    "# Plot SHAP summary plot for RandomForest (or any other model of choice)\n",
    "shap.summary_plot(shap_values[1], X_test_imputed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Predictions on Missing Loan Status\n",
    "And saving results to xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Predict on rows with missing loan_status and save predictions to Excel\n",
    "missing_loan_status = loan_data[loan_data['loan_status'].isnull()]\n",
    "predictions = models['RandomForest'].predict(missing_loan_status.drop(columns=['loan_status']))\n",
    "missing_loan_status['loan_status'] = predictions\n",
    "\n",
    "missing_loan_status.to_excel('LoanStatusPredictions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction Analysis Insights\n",
    "\n",
    "This section provides insights into the performance of three different machine learning models—**Random Forest**, **Logistic Regression**, and **K-Nearest Neighbors (KNN)**—based on their evaluation metrics.\n",
    "\n",
    "## 1. Random Forest Classifier\n",
    "\n",
    "### Results:\n",
    "- **Accuracy**: 0.9744 (97.44%)\n",
    "- **Confusion Matrix**:\n",
    "\n",
    "\n",
    "\n",
    "| True \\ Predicted | 0    | 1     | 2    |\n",
    "|------------------|------|-------|------|\n",
    "| **0**            | 3609 | 204   | 49   |\n",
    "| **1**            | 0    | 26347 | 361  |\n",
    "| **2**            | 23   | 249   | 3707 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Classification Report**:\n",
    "  - Precision for class 0: **0.99**\n",
    "  - Recall for class 0: **0.93**\n",
    "  - F1-score for class 0: **0.96**\n",
    "  - Precision for class 1: **0.98**\n",
    "  - Recall for class 1: **0.99**\n",
    "  - F1-score for class 1: **0.98**\n",
    "  - Precision for class 2: **0.90**\n",
    "  - Recall for class 2: **0.93**\n",
    "  - F1-score for class 2: **0.92**\n",
    "\n",
    "- **ROC AUC Score**: **0.9853**\n",
    "- **Weighted F1 Score**: **0.9744**\n",
    "\n",
    "### Insights:\n",
    "- The Random Forest model performs exceptionally well with an accuracy of about **97%**.\n",
    "- The confusion matrix shows that it correctly predicts most of the loan statuses, with very few false positives and false negatives.\n",
    "- The model has high precision and recall across all classes, particularly for class **1**, indicating it is very effective at identifying non-defaulted loans.\n",
    "- The ROC AUC score of **0.9853** suggests that the model has excellent discriminatory ability between classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic Regression\n",
    "\n",
    "### Results:\n",
    "- **Accuracy**: 0.9434 (94.34%)\n",
    "- **Confusion Matrix**:\n",
    "\n",
    "\n",
    "\n",
    "[[ 2563 1195 104]\n",
    "[ 0 26335 373]\n",
    "[ 19 263 3697]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Classification Report**:\n",
    "    - Precision for class 0: **0.99**\n",
    "    - Recall for class 0: **0.66**\n",
    "    - F1-score for class 0: **0.80**\n",
    "    - Precision for class 1: **0.95**\n",
    "    - Recall for class 1: **0.99**\n",
    "    - F1-score for class 1: **0.97**\n",
    "    - Precision for class 2: **0.89**\n",
    "    - Recall for class 2: **0.93**\n",
    "    - F1-score for class 2: **0.91**\n",
    "\n",
    "- **ROC AUC Score**: **0.9604**\n",
    "- **Weighted F1 Score**: **0.9404**\n",
    "\n",
    "### Insights:\n",
    "- Logistic Regression shows a good accuracy of about **94%**, but it is less effective than Random Forest.\n",
    "- The confusion matrix indicates that while it performs well in identifying non-defaulted loans (class **1**), it struggles with identifying defaults (class **0**) as evidenced by the lower recall (66%).\n",
    "- The precision and recall trade-off indicates that while it is good at predicting non-defaults, it may miss some actual defaults, leading to potential financial risk.\n",
    "- The ROC AUC score of **0.9604** still shows a strong ability to distinguish between classes but is not as robust as Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Results:\n",
    "- **Accuracy**: **85.93%**\n",
    "- **Confusion Matrix**:\n",
    "\n",
    "\n",
    "\n",
    "[[1458, 2333, 71]\n",
    "[136,26124, 448]\n",
    "[151,1722,2106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Classification Report**:\n",
    "    - Precision for class 0: **0.84**\n",
    "    - Recall for class 0: **0.38**\n",
    "    - F1-score for class 0: **0.52**\n",
    "    - Precision for class 1: **0.87**\n",
    "    - Recall for class 1: **0.98**\n",
    "    - F1-score for class 1: **0.92**\n",
    "    - Precision for class 2: **0.80**\n",
    "    - Recall for class 2: **0.53**\n",
    "    - F1-score for class2:**64**\n",
    "\n",
    "- **ROC AUC Score**: **0.8449**\n",
    "- **Weighted F1 Score**: **0.8416**\n",
    "\n",
    "### Insights:\n",
    "- KNN has the lowest accuracy of the three models at about **86%**, indicating that it struggles more than the others in predicting loan defaults accurately.\n",
    "- The confusion matrix reveals a significant number of misclassifications, particularly in identifying non-defaulted loans (class `0`), where recall is only `38%`. This means many actual defaults are being missed.\n",
    "- Although it performs reasonably well on the majority class (class `1`), its overall performance is not satisfactory due to poor precision and recall on classes `0` and `2`.\n",
    "- The ROC AUC score of `0.8449` indicates a fair ability to distinguish between classes but suggests that KNN may not be the best choice given its performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Comparison and Conclusion\n",
    "\n",
    "In summary:\n",
    "\n",
    "- The Random Forest model outperforms both Logistic Regression and KNN in terms of accuracy, precision, recall, and overall robustness in predictions.\n",
    "- Logistic Regression performs well but has limitations in identifying defaults effectively.\n",
    "- KNN shows significant weaknesses in accuracy and misclassification rates, making it less suitable compared to the other two models.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Given these insights:\n",
    "\n",
    "1. Consider using Random Forest as the primary model due to its high accuracy and robustness.\n",
    "2. If interpretability is crucial, Logistic Regression can be used alongside Random Forest to provide insights into feature importance.\n",
    "3. Further tuning of hyperparameters and possibly exploring ensemble methods could enhance model performance even more.\n",
    "4. Implement cross-validation techniques to ensure stability in model performance across different subsets of data.\n",
    "\n",
    "This analysis provides a clear pathway to selecting the best model while highlighting areas where improvements can be made in future iterations or additional data collection efforts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
